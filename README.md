# awesome-llm-reasoning
Collections of resources for LLM reasoning, including papers, codes and data. 
There are numbers of awesome repos focus on reasoning of LLM, we try to summarize these methods in a different angle, their specific domain,
such common reasoning, math, table related, KG relate or multimodal(welcome to name them more).

*We try to give a comprehensive view of resources, thus papers here may not be well evaluated by now for the sake of time, any suggestion are well appreciated!*

[![Awesome](https://awesome.re/badge.svg)](https://github.com/4IK1d/awesome-llm-reasoning/) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
![](https://img.shields.io/github/last-commit/4IK1d/awesome-llm-reasoning?color=green) 
![](https://img.shields.io/badge/PRs-Welcome-red)


# Table-based Reasoning

| Paper | code | data |
| :---------------------------------------------- | :---------------------------------------------- | :---------------------------------------------- |
|  [StructGPT: A General Framework for Large Language Model to Reason over Structured Data](https://arxiv.org/pdf/2305.09645.pdf)  | [code](https://github.com/RUCAIBox/StructGPT) | [data](https://drive.google.com/drive/folders/11_2pqU_MhEtmxpp3zfK_8EJ1bbQzsnfJ?usp=sharing) / [raw](https://github.com/HKUNLP/UnifiedSKG) |
|  [Large Language Models are Effective Table-to-Text Generators, Evaluators, and Feedback Providers](https://arxiv.org/abs/2305.14987)  | [code](https://github.com/yilunzhao/LLM-T2T) | [data](https://github.com/yilunzhao/LLM-T2T) |
|  [Large Language Models are few(1)-shot Table Reasoners](https://arxiv.org/pdf/2210.06710.pdf)  | [code](https://github.com/wenhuchen/TableCoT) | [WikiTableQuestions](https://ppasupat.github.io/WikiTableQuestions/) / [FetaQA](https://arxiv.org/pdf/2104.00369v1.pdf) / [TabFact](https://tabfact.github.io/) / [FEVEROUS](https://fever.ai/dataset/feverous.html)|
|  [OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering](https://arxiv.org/pdf/2207.03637.pdf)  | [code](https://github.com/jzbjyb/OmniTab) | [data](https://drive.google.com/drive/u/1/folders/14IAqJb9ObVDE5oOJouhkqgd_mn11PkYY) |
|  [REASTAP: Injecting Table Reasoning Skills During Pre-training via Synthetic Reasoning Examples](https://arxiv.org/pdf/2210.12374.pdf)  | [code](https://github.com/Yale-LILY/ReasTAP) | [data](https://drive.google.com/drive/folders/1YRmRibz_fVZbrb2W1ynFWS6h-uwJw0oN?usp=sharing) |
|  [Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning](https://arxiv.org/pdf/2301.13808)  | [code](https://github.com/AlibabaResearch/DAMO-ConvAI) | [WikiTableQuestions](https://drive.google.com/drive/u/1/folders/14IAqJb9ObVDE5oOJouhkqgd_mn11PkYY) / [FetaQA](https://arxiv.org/pdf/2104.00369v1.pdf) |


